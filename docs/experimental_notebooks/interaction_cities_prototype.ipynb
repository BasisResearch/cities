{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer.autoguide import AutoNormal, AutoMultivariateNormal, AutoDelta\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from chirho.indexed.handlers import IndexPlatesMessenger\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "from chirho.indexed.ops import IndexSet, gather, indices_of\n",
    "from chirho.interventional.handlers import do\n",
    "from chirho.observational.handlers import condition\n",
    "\n",
    "\n",
    "from cities.utils.data_grabber import (DataGrabber, list_available_features, list_tensed_features)\n",
    "from cities.utils.cleaning_utils import check_if_tensed \n",
    "\n",
    "from pyro.infer import Predictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictive(model, data, intervention_variable, outcome_dataset, forward_shift, guide = None, **kwargs):\n",
    "\n",
    "    model = condition(data=dict(**kwargs))(model)\n",
    "    \n",
    "    # \n",
    "    #if guide:\n",
    "    #    guide_tr = pyro.poutine.trace(guide).get_trace()\n",
    "    #    model_tr = pyro.poutine.trace(pyro.poutine.replay(model(T_outcome = T_outcome, N_states = N_states, state_index = state_index,\n",
    "    #                            time_index = time_index, intervention = intervention), trace=guide_tr)).get_trace()\n",
    "    #else:\n",
    "    with pyro.poutine.trace() as tr:\n",
    "        model(**kwargs)\n",
    "\n",
    "    model_tr = tr.trace\n",
    "    y = model_tr.nodes['y']['value']\n",
    "\n",
    "    predictive_copy = data.copy()\n",
    "\n",
    "    predictive_copy['post_test'] = y.reshape(-1, 1).detach().numpy()  \n",
    "\n",
    "    sns.pairplot(predictive_copy[[intervention_variable, f\"{outcome_dataset}_shifted_by_{forward_shift}\", 'post_test']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg = DataGrabber()\n",
    "\n",
    "#original development settings\n",
    "outcome_dataset = 'unemployment_rate' #outcome variable = its dataset name\n",
    "intervention_dataset = 'spending_commerce'\n",
    "intervention_variable = 'total_obligated_amount' #intervention variable\n",
    "forward_shift = 2 # how many discrete steps forward we'll be trying to predict\n",
    "\n",
    "#second tested setting\n",
    "# outcome_dataset = 'gdp' #outcome variable = its dataset name\n",
    "# intervention_dataset = 'spending_commerce'\n",
    "# intervention_variable = 'total_obligated_amount' #intervention variable\n",
    "# forward_shift = 2 # how many discrete steps forward we'll be trying to predict\n",
    "\n",
    "\n",
    "#third tested setting\n",
    "# outcome_dataset = 'gdp' #outcome variable = its dataset name\n",
    "# intervention_dataset = 'spending_transportation'\n",
    "# intervention_variable = 'total_obligated_amount' #intervention variable\n",
    "# forward_shift = 1 # how many discrete steps forward we'll be trying to predict\n",
    "\n",
    "\n",
    "tensed_covariates_datasets = [var for var in list_tensed_features() if var not in [outcome_dataset, intervention_dataset]]\n",
    "fixed_covariates_datasets = [var for var in list_available_features() if var not in tensed_covariates_datasets + [outcome_dataset, intervention_dataset]]\n",
    "\n",
    "# at least for now we will use only fixed covariates\n",
    "# as other time series have different time spans, year missingness etc.\n",
    "\n",
    "# morever you don't want to use time series from after the first recorded intervention\n",
    "# so what we could consider doing is adding info from the time series\n",
    "# for years preceding the first recorded intervention year\n",
    "# but that's for later\n",
    "\n",
    "dg.get_features_std_long(list_available_features()) \n",
    "dg.get_features_std_wide(list_available_features()) \n",
    "\n",
    "# we need to focus on years for which we have data on both the intervention and outcome\n",
    "# for now, find the boundary years\n",
    "\n",
    "year_min = max(dg.std_long[intervention_dataset]['Year'].min(), dg.std_long[outcome_dataset]['Year'].min())\n",
    "year_max = min(dg.std_long[intervention_dataset]['Year'].max(), dg.std_long[outcome_dataset]['Year'].max())\n",
    "\n",
    "outcome_df = dg.std_long[outcome_dataset].sort_values(by=['GeoFIPS', 'Year'])\n",
    "\n",
    "\n",
    "# now we adding forward shift to the outcome\n",
    "# cleaning up and puting intervention/outcome in one df\n",
    "# and fixed covariates in another\n",
    "\n",
    "outcome_df[f'{outcome_dataset}_shifted_by_{forward_shift}'] = None\n",
    "\n",
    "geo_subsets = []\n",
    "for geo_fips in outcome_df['GeoFIPS'].unique():\n",
    "    geo_subset = outcome_df[outcome_df['GeoFIPS'] == geo_fips].copy()\n",
    "    # Shift the 'Value' column `forward_shift` in a new column\n",
    "    geo_subset[f'{outcome_dataset}_shifted_by_{forward_shift}'] = geo_subset['Value'].shift(-forward_shift)\n",
    "    geo_subsets.append(geo_subset)\n",
    "    \n",
    "outcome_df = pd.concat(geo_subsets).reset_index(drop=True)\n",
    "\n",
    "\n",
    "outcome = outcome_df[(outcome_df['Year'] >= year_min) & (outcome_df['Year'] <= year_max + forward_shift)]\n",
    "intervention = dg.std_long[intervention_dataset][(dg.std_long[intervention_dataset]['Year'] >= year_min) & (dg.std_long[intervention_dataset]['Year'] <= year_max)]    \n",
    "f_covariates = {dataset: dg.std_wide[dataset] for dataset in fixed_covariates_datasets}\n",
    "f_covariates_joint = f_covariates[fixed_covariates_datasets[0]]\n",
    "for dataset in f_covariates.keys():\n",
    "    if dataset != fixed_covariates_datasets[0]:\n",
    "        if 'GeoName' in f_covariates[dataset].columns:\n",
    "            f_covariates[dataset] = f_covariates[dataset].drop(columns=['GeoName'])\n",
    "        f_covariates_joint = f_covariates_joint.merge(f_covariates[dataset], on=['GeoFIPS'])\n",
    "\n",
    "\n",
    "i_o_data = pd.merge(outcome, intervention, on=['GeoFIPS', 'Year'])\n",
    "\n",
    "if 'GeoName_x' in i_o_data.columns:\n",
    "    i_o_data.rename(columns={'GeoName_x': \"GeoName\"}, inplace=True)    \n",
    "    columns_to_drop = i_o_data.filter(regex=r'^GeoName_[a-zA-Z]$')\n",
    "    i_o_data.drop(columns=columns_to_drop.columns, inplace=True)\n",
    "\n",
    "i_o_data.rename(columns={'Value': outcome_dataset}, inplace=True)\n",
    "\n",
    "i_o_data['state'] = [code // 1000 for code in i_o_data['GeoFIPS']]\n",
    "\n",
    "N_s = len(i_o_data['state'].unique())  #number of states\n",
    "i_o_data.dropna(inplace=True)\n",
    "\n",
    "\n",
    "i_o_data['unit_index']= pd.factorize(i_o_data['GeoFIPS'].values)[0]\n",
    "i_o_data['state_index']= pd.factorize(i_o_data['state'].values)[0]\n",
    "i_o_data['time_index']= pd.factorize(i_o_data['Year'].values)[0]\n",
    "\n",
    "\n",
    "assert i_o_data['GeoFIPS'].isin(f_covariates_joint['GeoFIPS']).all()\n",
    "\n",
    "f_covariates_joint.drop(columns=['GeoName'], inplace=True)\n",
    "data = i_o_data.merge(f_covariates_joint, on='GeoFIPS', how='left')\n",
    "\n",
    "assert not data.isna().any().any()\n",
    "\n",
    "time_index_idx = data.columns.get_loc('time_index')\n",
    "covariates_df = data.iloc[:, time_index_idx + 1:].copy()\n",
    "covariates_df_sparse = covariates_df.copy()\n",
    "covariates_df_sparse['unit_index'] = data['unit_index']\n",
    "covariates_df_sparse['state_index'] = data['state_index']\n",
    "covariates_df_sparse.drop_duplicates(inplace=True)\n",
    "assert set(covariates_df_sparse['unit_index']) == set(data['unit_index'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tensors for modeling\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "y = data[f'{outcome_dataset}_shifted_by_{forward_shift}']\n",
    "y = torch.tensor(y, dtype=torch.float32, device = device) \n",
    "\n",
    "unit_index = torch.tensor(data['unit_index'], dtype=torch.int, device = device)\n",
    "unit_index_sparse = torch.tensor(covariates_df_sparse['unit_index'], dtype=torch.int)\n",
    "\n",
    "state_index = torch.tensor(data['state_index'], dtype=torch.int, device = device)\n",
    "state_index_sparse = torch.tensor(covariates_df_sparse['state_index'], dtype=torch.int)\n",
    "\n",
    "time_index = torch.tensor(data['time_index'], dtype=torch.int, device = device)\n",
    "intervention = torch.tensor(data[intervention_variable], dtype=torch.float32, device = device)\n",
    "\n",
    "covariates = torch.tensor(covariates_df.values, dtype = torch.float32, device = device)\n",
    "\n",
    "covariates_df_sparse.drop(columns=['unit_index','state_index'], inplace=True)\n",
    "covariates_sparse = torch.tensor(covariates_df_sparse.values, dtype = torch.float32, device = device)\n",
    "\n",
    "N_cov = covariates.shape[1] #number of covariates\n",
    "N_u = covariates_sparse.shape[0] #number of units (counties)\n",
    "N_obs = len(y) #number of observations\n",
    "N_t = len(time_index.unique()) #number of time points\n",
    "N_s = len(state_index.unique()) #number of states\n",
    "\n",
    "assert len(intervention) == len(y)\n",
    "assert len(unit_index) == len(y)\n",
    "assert len(state_index) == len(unit_index)\n",
    "assert len(time_index) == len(unit_index)\n",
    "assert covariates.shape[1] == covariates_sparse.shape[1]\n",
    "assert len(unit_index_sparse) == N_u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_bias torch.Size([30, 1, 1, 1])\n",
      "UsX torch.Size([30, 51, 1, 1])\n",
      "UsX_weight_selected torch.Size([30, 2998])\n",
      "X_means torch.Size([2998, 30])\n",
      "X torch.Size([2998, 30])\n",
      "X_bias (30, 1, 1, 1)\n",
      "UsX (30, 51, 1, 1)\n",
      "UsX_weight_selected (30, 2998)\n",
      "X_means (2998, 30)\n",
      "X (2998, 30)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"1695pt\" height=\"322pt\"\n",
       " viewBox=\"0.00 0.00 1695.30 322.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 318)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-318 1691.3,-318 1691.3,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_covariates_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"442.3,-132.5 442.3,-246.5 968.3,-246.5 968.3,-132.5 442.3,-132.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"901.8\" y=\"-140.3\" font-family=\"Times,serif\" font-size=\"14.00\">covariates_plate</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_states_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"812.3,-163.5 812.3,-238.5 960.3,-238.5 960.3,-163.5 812.3,-163.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"908.8\" y=\"-171.3\" font-family=\"Times,serif\" font-size=\"14.00\">states_plate</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_counties_plate</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"703.3,-8 703.3,-83 822.3,-83 822.3,-8 703.3,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"762.8\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">counties_plate</text>\n",
       "</g>\n",
       "<g id=\"clust4\" class=\"cluster\">\n",
       "<title>cluster_states_plate__CLONE</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"1198.3,-163.5 1198.3,-238.5 1495.3,-238.5 1495.3,-163.5 1198.3,-163.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"1443.8\" y=\"-171.3\" font-family=\"Times,serif\" font-size=\"14.00\">states_plate</text>\n",
       "</g>\n",
       "<!-- Y_bias -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Y_bias</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"40.3\" cy=\"-212.5\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">Y_bias</text>\n",
       "</g>\n",
       "<!-- T_bias -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>T_bias</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"139.3\" cy=\"-212.5\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"139.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">T_bias</text>\n",
       "</g>\n",
       "<!-- weight_TY -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>weight_TY</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"256.3\" cy=\"-212.5\" rx=\"58.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_TY</text>\n",
       "</g>\n",
       "<!-- sigma_X -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>sigma_X</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"382.3\" cy=\"-212.5\" rx=\"49.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"382.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_X</text>\n",
       "</g>\n",
       "<!-- X -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>X</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"762.3\" cy=\"-57\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"762.3\" y=\"-53.3\" font-family=\"Times,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- sigma_X&#45;&gt;X -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>sigma_X&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M386.74,-194.5C393.67,-171.58 409.37,-131.32 438.3,-111 484.03,-78.88 649.82,-64.87 724.9,-60.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"725.53,-63.54 735.29,-59.43 725.1,-56.55 725.53,-63.54\"/>\n",
       "</g>\n",
       "<!-- sigma_T -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>sigma_T</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1025.3\" cy=\"-212.5\" rx=\"48.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1025.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_T</text>\n",
       "</g>\n",
       "<!-- sigma_Y -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>sigma_Y</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1140.3\" cy=\"-212.5\" rx=\"48.99\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1140.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_Y</text>\n",
       "</g>\n",
       "<!-- X_bias -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>X_bias</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"762.3\" cy=\"-212.5\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"762.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">X_bias</text>\n",
       "</g>\n",
       "<!-- X_bias&#45;&gt;X -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>X_bias&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M762.3,-194.49C762.3,-168.09 762.3,-117.03 762.3,-85.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"765.8,-85.09 762.3,-75.09 758.8,-85.09 765.8,-85.09\"/>\n",
       "</g>\n",
       "<!-- weight_XT -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>weight_XT</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"645.3\" cy=\"-212.5\" rx=\"59.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"645.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_XT</text>\n",
       "</g>\n",
       "<!-- weight_XY -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>weight_XY</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"509.3\" cy=\"-212.5\" rx=\"59.29\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"509.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_XY</text>\n",
       "</g>\n",
       "<!-- weight_UsX -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>weight_UsX</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"886.3\" cy=\"-212.5\" rx=\"65.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"886.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_UsX</text>\n",
       "</g>\n",
       "<!-- weight_UsX&#45;&gt;X -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>weight_UsX&#45;&gt;X</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M875.8,-194.44C862.84,-173.87 839.63,-138.61 816.3,-111 806.93,-99.92 795.59,-88.53 785.73,-79.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"787.9,-76.4 778.2,-72.13 783.12,-81.52 787.9,-76.4\"/>\n",
       "</g>\n",
       "<!-- weight_UsT -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>weight_UsT</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1421.3\" cy=\"-212.5\" rx=\"65.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1421.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_UsT</text>\n",
       "</g>\n",
       "<!-- weight_UsY -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>weight_UsY</title>\n",
       "<ellipse fill=\"white\" stroke=\"black\" cx=\"1272.3\" cy=\"-212.5\" rx=\"65.79\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"1272.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_UsY</text>\n",
       "</g>\n",
       "<!-- distribution_description_node -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>distribution_description_node</title>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-298.8\" font-family=\"Times,serif\" font-size=\"14.00\">Y_bias ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-283.8\" font-family=\"Times,serif\" font-size=\"14.00\">T_bias ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-268.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_TY ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-253.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_X ~ Exponential</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-238.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_T ~ Exponential</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-223.8\" font-family=\"Times,serif\" font-size=\"14.00\">sigma_Y ~ Exponential</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-208.8\" font-family=\"Times,serif\" font-size=\"14.00\">X_bias ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-193.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_XT ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-178.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_XY ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-163.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_UsT ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-148.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_UsY ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-133.8\" font-family=\"Times,serif\" font-size=\"14.00\">weight_UsX ~ Normal</text>\n",
       "<text text-anchor=\"start\" x=\"1513.3\" y=\"-118.8\" font-family=\"Times,serif\" font-size=\"14.00\">X ~ Normal</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f6771006b30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leeway = .4\n",
    "\n",
    "\n",
    "def cities_model_F(N_t, N_s, state_index, time_index):\n",
    "    \n",
    "    Y_bias = pyro.sample(\"Y_bias\", dist.Normal(0, leeway))\n",
    "    T_bias = pyro.sample(\"T_bias\", dist.Normal(0, leeway))\n",
    "\n",
    "    weight_TY = pyro.sample(\"weight_TY\", dist.Normal(0, leeway))\n",
    "    \n",
    "    sigma_X = pyro.sample(\"sigma_X\", dist.Exponential(1))\n",
    "    sigma_T = pyro.sample(\"sigma_T\", dist.Exponential(1))\n",
    "    sigma_Y = pyro.sample(\"sigma_Y\", dist.Exponential(1))\n",
    "\n",
    "    observations_plate = pyro.plate(\"observations_plate\", N_obs, dim = -1)\n",
    "    counties_plate = pyro.plate(\"counties_plate\", N_u, dim = -2)\n",
    "    states_plate = pyro.plate(\"states_plate\", N_s, dim=-3)\n",
    "    covariates_plate = pyro.plate(\"covariates_plate\", N_cov, dim=-4)\n",
    "    time_plate = pyro.plate(\"time_plate\", N_t, dim=-5)\n",
    "\n",
    "\n",
    "    with covariates_plate:\n",
    "        X_bias = pyro.sample(\"X_bias\", dist.Normal(0, leeway))\n",
    "        print(\"X_bias\", X_bias.shape)\n",
    "        weight_XT = pyro.sample(\"weight_XT\", dist.Normal(0, leeway))\n",
    "        weight_XY = pyro.sample(\"weight_XY\", dist.Normal(0, leeway))\n",
    "\n",
    "\n",
    "    with states_plate:\n",
    "        weight_UsT = pyro.sample(\"weight_UsT\", dist.Normal(0, leeway))        \n",
    "        weight_UsY = pyro.sample(\"weight_UsY\", dist.Normal(0, leeway))        \n",
    "\n",
    "        with covariates_plate:\n",
    "            weight_UsX = pyro.sample(\"weight_UsX\", dist.Normal(0, leeway)) \n",
    "\n",
    "    print(\"UsX\", weight_UsX.shape)\n",
    "    with counties_plate:\n",
    "        # squeezing added to avoid shape errors when predictive sampling\n",
    "        # as values get dimensions of size 1 (on both sides of non-trival dims)\n",
    "        #UsX_weighted = torch.einsum(\"ij...,j...->ij\", weight_UsX.squeeze(), Us.squeeze())\n",
    "        UsX_weight_selected = weight_UsX.squeeze()[...,state_index_sparse].squeeze()\n",
    "        print(\"UsX_weight_selected\", UsX_weight_selected.shape)\n",
    "        X_means = torch.einsum(\"i,ik->ik\" ,X_bias.squeeze(), UsX_weight_selected)[...,unit_index_sparse].T\n",
    "        print(\"X_means\", X_means.shape)\n",
    "        X = pyro.sample(\"X\", dist.Normal(X_means, sigma_X))\n",
    "        print(\"X\", X.shape)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "cities_model_F(N_t, N_s, state_index, time_index)\n",
    "\n",
    "pyro.render_model(cities_model_F, \n",
    "                  model_args=(N_t, N_s, state_index, time_index), render_distributions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
