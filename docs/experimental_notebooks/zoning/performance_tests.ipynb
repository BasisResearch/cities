{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "from cities.utils.data_grabber import find_repo_root\n",
    "from cities.utils.data_loader import ZoningDataset\n",
    "from cities.modeling.evaluation import prep_data_for_test, test_performance\n",
    "\n",
    "from cities.modeling.simple_linear import SimpleLinear\n",
    "from cities.modeling.svi_inference import run_svi_inference\n",
    "from pyro.infer import Predictive\n",
    "from chirho.robust.handlers.predictive import PredictiveModel\n",
    "\n",
    "\n",
    "\n",
    "root = find_repo_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "to_map = torch.tensor([0,10, 20, 10, 0])\n",
    "map = {0:0, 10:1, 20:2}\n",
    "print([map[x.item()] for x in to_map])\n",
    "#mapped = torch.tensor([map[x] for x in to_map])\n",
    "#print(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([61]), torch.Size([61])]\n",
      "[tensor([0, 1, 2]), tensor([ 0,  2,  3,  5,  6,  7,  8, 10, 11, 12, 16, 17, 18, 21, 22, 24, 26, 30,\n",
      "        32, 33, 37, 38, 40, 42, 52, 58, 59, 62, 64])]\n",
      "neighborhood_id\n",
      "original tensor([ 6, 32, 21,  3,  8, 11, 32,  5,  7,  6, 42, 30,  7,  3,  7, 18,  2, 38,\n",
      "        59, 17, 10,  8, 37,  0,  7, 22,  7,  8, 12, 18, 16,  7,  0, 52, 42,  7,\n",
      "        32, 32,  7, 64,  8, 16, 62, 33,  3, 32, 11, 12,  3, 42, 17, 12, 58,  7,\n",
      "         2, 17, 24, 26, 32, 40,  3])\n",
      "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 6: 5, 7: 6, 8: 7, 9: 8, 10: 9, 11: 10, 12: 11, 13: 12, 14: 13, 15: 14, 17: 15, 18: 16, 19: 17, 20: 18, 21: 19, 22: 20, 23: 21, 24: 22, 25: 23, 27: 24, 28: 25, 29: 26, 30: 27, 31: 28, 32: 29, 33: 30, 34: 31, 35: 32, 36: 33, 37: 34, 38: 35, 39: 36, 40: 37, 41: 38, 42: 39, 43: 40, 44: 41, 45: 42, 46: 43, 47: 44, 48: 45, 49: 46, 50: 47, 51: 48, 53: 49, 54: 50, 55: 51, 56: 52, 57: 53, 60: 54, 61: 55, 62: 56, 63: 57}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(mappings[name])\n\u001b[1;32m     41\u001b[0m _train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([mappings[name][x\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m  _train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name]])\n\u001b[0;32m---> 42\u001b[0m _test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([mappings[name][x\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m  _test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name]])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, _test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name])\n",
      "Cell \u001b[0;32mIn[44], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(mappings[name])\n\u001b[1;32m     41\u001b[0m _train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([mappings[name][x\u001b[38;5;241m.\u001b[39mitem()] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m  _train_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name]])\n\u001b[0;32m---> 42\u001b[0m _test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[43mmappings\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m  _test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name]])\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecoded\u001b[39m\u001b[38;5;124m\"\u001b[39m, _test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m][name])\n",
      "\u001b[0;31mKeyError\u001b[0m: 5"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, categorical_levels = prep_data_for_test()\n",
    "\n",
    "kwarg_names = {\"categorical\": ['limit_id', 'neighborhood_id'],\n",
    "              \"continuous\": {'parcel_area'}, \"outcome\": 'housing_units'}\n",
    "\n",
    "model_class = SimpleLinear\n",
    "n_steps=600 \n",
    "plot = True\n",
    "\n",
    "\n",
    "assert all(item in kwarg_names.keys() for item in [\"categorical\", \"continuous\", \"outcome\"])\n",
    "assert kwarg_names[\"outcome\"] not in kwarg_names[\"continuous\"]\n",
    "\n",
    "train_data = next(iter(train_loader))\n",
    "test_data = next(iter(test_loader))\n",
    "\n",
    "_train_data = {}\n",
    "_train_data['outcome'] = train_data['continuous'][kwarg_names['outcome']]\n",
    "_train_data['categorical'] = {key: val for key, val in train_data['categorical'].items() if key in kwarg_names['categorical']}\n",
    "_train_data['continuous'] = {key: val for key, val in train_data['continuous'].items() if key in kwarg_names['continuous']}\n",
    "\n",
    "_test_data = {}\n",
    "_test_data['outcome'] = test_data['continuous'][kwarg_names['outcome']]\n",
    "_test_data['categorical'] = {key: val for key, val in test_data['categorical'].items() if key in kwarg_names['categorical']}\n",
    "_test_data['continuous'] = {key: val for key, val in test_data['continuous'].items() if key in kwarg_names['continuous']}\n",
    "\n",
    "#_train_data['categorical'] = {key: val for key, val in train_data['categorical'].items() if key in kwarg_names['categorical']}\n",
    "#_train_data['continuous'] = {key: val for key, val in train_data['continuous'].items() if key in kwarg_names['continuous']}\n",
    "print([_test_data['categorical'][name].shape for name in kwarg_names[\"categorical\"]])\n",
    "print([_test_data['categorical'][name].unique() for name in kwarg_names[\"categorical\"]])\n",
    "\n",
    "#print(_train_data['outcome'])\n",
    "\n",
    "\n",
    "##########################\n",
    "# eliminate test categories not in the training data\n",
    "##########################\n",
    "def apply_mask(data, mask):\n",
    "        return {key: val[mask] for key, val in data.items()}\n",
    "\n",
    "mask = torch.ones(len(test_data['outcome']), dtype=torch.bool)\n",
    "for key, value in test_data['categorical'].items():\n",
    "    mask = mask *  torch.isin(test_data['categorical'][key],(train_data['categorical'][key].unique()))\n",
    "\n",
    "test_data['categorical'] = apply_mask(test_data['categorical'], mask)\n",
    "test_data['continuous'] = apply_mask(test_data['continuous'], mask)\n",
    "test_data['outcome'] = test_data['outcome'][mask]\n",
    "\n",
    "for key in test_data['categorical'].keys():\n",
    "        assert test_data['categorical'][key].shape[0] == mask.sum()\n",
    "for key in test_data['continuous'].keys():\n",
    "    assert test_data['continuous'][key].shape[0] == mask.sum()\n",
    "\n",
    "# raise error if sum(mask) < .5 * len(test_data['outcome'])\n",
    "if sum(mask) < .5 * len(test_data['outcome']):\n",
    "    raise ValueError(\"Sampled test data has too many new categorical levels, consider decreasing train size\")\n",
    "\n",
    "\n",
    "mappings = {}\n",
    "for name in _train_data['categorical'].keys():\n",
    "    print(name)\n",
    "    print(\"original\", _test_data['categorical'][name])\n",
    "    unique_train = torch.unique(_train_data['categorical'][name])\n",
    "    mappings[name] = {v.item(): i for i, v in enumerate(unique_train)}\n",
    "    print(mappings[name])\n",
    "    _train_data['categorical'][name] = torch.tensor([mappings[name][x.item()] for x in  _train_data['categorical'][name]])\n",
    "    _test_data['categorical'][name] = torch.tensor([mappings[name][x.item()] for x in  _test_data['categorical'][name]])\n",
    "    print(\"recoded\", _test_data['categorical'][name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_performance(SimpleLinear, kwarg_names, train_loader,\n",
    "                  test_loader, categorical_levels)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polis-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
