{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import dill\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pyro\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from chirho.indexed.ops import IndexSet, gather\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import pyro\n",
    "from pyro.infer import Predictive\n",
    "\n",
    "from chirho.counterfactual.handlers import MultiWorldCounterfactual\n",
    "\n",
    "# from cities.modeling.zoning_models.units_causal_model import UnitsCausalModel\n",
    "from cities.modeling.zoning_models.distance_causal_model import DistanceCausalModel\n",
    "from cities.modeling.svi_inference import run_svi_inference\n",
    "from cities.utils.data_loader import select_from_data\n",
    "\n",
    "\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "\n",
    "from cities.modeling.evaluation import (\n",
    "    prep_data_for_test,\n",
    "    test_performance,\n",
    ")\n",
    "\n",
    "from cities.modeling.zoning_models.tracts_model import TractsModel\n",
    "\n",
    "from cities.modeling.svi_inference import run_svi_inference\n",
    "from pyro.infer import Predictive\n",
    "from chirho.observational.handlers.predictive import PredictiveModel\n",
    "from chirho.interventional.handlers import do\n",
    "\n",
    "\n",
    "smoke_test = \"CI\" in os.environ\n",
    "\n",
    "# use when testing model health\n",
    "# smoke_test = True\n",
    "\n",
    "n_steps = 10 if smoke_test else 1500\n",
    "num_samples = 10 if smoke_test else 1000\n",
    "\n",
    "from cities.utils.data_grabber import find_repo_root\n",
    "\n",
    "root = find_repo_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['housing_units', 'housing_units_original', 'income', 'income_original', 'mean_distance', 'mean_distance_original', 'mean_limit', 'mean_limit_original', 'median_distance', 'median_distance_original', 'median_value', 'median_value_original', 'segregation', 'segregation_original', 'total_value', 'total_value_original', 'white', 'white_original'])\n",
      "dict_keys(['year_original', 'year', 'census_tract'])\n",
      "dict_keys(['housing_units', 'income', 'mean_limit_original', 'median_distance', 'median_value', 'segregation_original', 'total_value', 'white_original'])\n",
      "torch.Size([816])\n",
      "torch.Size([816])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "census_tracts_data_path = os.path.join(root, \"data/minneapolis/processed/census_tracts_dataset.pt\")\n",
    "\n",
    "\n",
    "ct_dataset_read = torch.load(census_tracts_data_path)\n",
    "\n",
    "ct_loader = DataLoader(\n",
    "    ct_dataset_read, batch_size=len(ct_dataset_read), shuffle=True\n",
    ")\n",
    "\n",
    "data = next(iter(ct_loader))\n",
    "\n",
    "print(data[\"continuous\"].keys())\n",
    "print(data['categorical'].keys())   \n",
    "\n",
    "kwargs = {\n",
    "    \"categorical\": [\n",
    "        \"year_original\",\n",
    "        \"year\",\n",
    "        \"census_tract\"\n",
    "    ],\n",
    "    \"continuous\": {\n",
    "      'housing_units',\n",
    "      'total_value',\n",
    "      'median_value',\n",
    "      'mean_limit_original',\n",
    "      'median_distance',\n",
    "      'income',\n",
    "      'segregation_original',\n",
    "      'white_original',\n",
    "    },\n",
    "    'outcome': 'housing_units'\n",
    "}\n",
    "\n",
    "subset = select_from_data(data, kwargs)\n",
    "print(subset[\"continuous\"].keys())\n",
    "print(subset['continuous']['housing_units'].shape)\n",
    "print(subset['categorical']['census_tract'].shape)\n",
    "\n",
    "tracts_model = TractsModel(\n",
    "    **subset, categorical_levels=ct_dataset_read.categorical_levels\n",
    ")\n",
    "\n",
    "\n",
    "subset_for_preds = copy.deepcopy(subset)\n",
    "subset_for_preds['continuous']['housing_units'] = None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "guide_path = os.path.join(root, \"data/minneapolis/guides\", \"tracts_model_guide.pkl\")\n",
    "\n",
    "param_path = os.path.join(root, \"data/minneapolis/guides\", \"tracts_model_params.pth\")\n",
    "\n",
    "with open(guide_path, \"rb\") as file:\n",
    "    guide = dill.load(file)\n",
    "\n",
    "pyro.get_param_store().load(param_path)\n",
    "\n",
    "predictive = Predictive(\n",
    "    model=tracts_model, guide=guide, num_samples=num_samples, parallel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONCE TO GENERATE BACKGROUND DATA USED FOR INTERVENTION CONSTRUCTION\n",
    "\n",
    "# data  = copy.deepcopy(values[['limit_con', 'downtown_yn', 'year', 'distance_to_transit',\n",
    "#                              'parcel', 'census_tract']])\n",
    "\n",
    "# data.to_csv(os.path.join(root, \"data/minneapolis/processed/census_tract_intervention_required.csv\"))\n",
    "\n",
    "# census_ids = pd.DataFrame({'census_tract':subset['categorical']['census_tract'].numpy(), \n",
    "#                            'year':subset['categorical']['year_original'].numpy()})\n",
    "# census_ids.to_csv(os.path.join(root, \"data/minneapolis/processed/census_ids.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(os.path.join(root,\n",
    "#              \"data/minneapolis/processed/census_tract_intervention_required.csv\"))\n",
    "\n",
    "# census_ids = pd.read_csv(os.path.join(root, \"data/minneapolis/processed/census_ids.csv\"))\n",
    "\n",
    "# display(data.head())\n",
    "# display(census_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run values_intervention 1:  0.7421717643737793\n",
      "Time to run values_intervention 2:  0.056528329849243164\n",
      "Time to run values_intervention 3:  0.054033756256103516\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def values_intervention(radius_blue, limit_blue, \n",
    "                radius_yellow, limit_yellow, reform_year = 2015):\n",
    "    \n",
    "\n",
    "    # don't want to load large data multiple times\n",
    "    \n",
    "    if not hasattr(values_intervention, \"global_census_ids\"):   \n",
    "        values_intervention.global_census_ids = pd.read_csv(os.path.join(root, \"data/minneapolis/processed/census_ids.csv\"))\n",
    "\n",
    "        values_intervention.global_data = pd.read_csv(os.path.join(root,\n",
    "            \"data/minneapolis/processed/census_tract_intervention_required.csv\"))\n",
    "\n",
    "        data = values_intervention.global_data\n",
    "        census_ids = values_intervention.global_census_ids    \n",
    "        values_intervention.global_data = data[\n",
    "            (data['census_tract'].isin(census_ids['census_tract'])) & \n",
    "            (data['year'].isin(census_ids['year']))]\n",
    "        \n",
    "    data = values_intervention.global_data.copy()\n",
    "\n",
    "    intervention = copy.deepcopy(values_intervention.global_data['limit_con'])\n",
    "    downtown = data['downtown_yn']\n",
    "    new_blue = (~downtown) & (data['year'] >= reform_year) & (data[\"distance_to_transit\"] <= radius_blue)\n",
    "    new_yellow = (\n",
    "            (~downtown)\n",
    "            & (data[\"year\"] >= reform_year)\n",
    "            & (data[\"distance_to_transit\"] > radius_blue)\n",
    "            & (data[\"distance_to_transit\"] <= radius_yellow)\n",
    "        )\n",
    "    new_other = (\n",
    "        (~downtown) & (data['year'] > reform_year) & (data[\"distance_to_transit\"] > radius_yellow)\n",
    "    )\n",
    "\n",
    "    intervention[downtown] = 0.0\n",
    "    intervention[new_blue] = limit_blue\n",
    "    intervention[new_yellow] = limit_yellow\n",
    "    intervention[new_other] = 1.0\n",
    "\n",
    "    data['intervention'] = intervention \n",
    "\n",
    "    return data\n",
    "\n",
    "#note subsequent runs are much faster\n",
    "start = time.time()\n",
    "simple_intervention = values_intervention(300, .5, 700, .7, reform_year = 2015)\n",
    "end = time.time()\n",
    "print(\"Time to run values_intervention 1: \", end - start)\n",
    "start2 = time.time()\n",
    "simple_intervention2 = values_intervention(400, .5, 800, .6, reform_year = 2013)\n",
    "end2 = time.time()\n",
    "\n",
    "print(\"Time to run values_intervention 2: \", end2 - start2)\n",
    "\n",
    "\n",
    "start3 = time.time()\n",
    "simple_intervention3 = values_intervention(200, .4, 1000, .65, reform_year = 2013)\n",
    "end3 = time.time()\n",
    "\n",
    "\n",
    "print(\"Time to run values_intervention 3: \", end3 - start3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run tracts_intervention 1:  0.14096927642822266\n",
      "Time to run tracts_intervention 2:  0.1297438144683838\n"
     ]
    }
   ],
   "source": [
    "def tracts_intervention (radius_blue, limit_blue, \n",
    "                radius_yellow, limit_yellow, reform_year = 2015):\n",
    "    \n",
    "    parcel_interventions = values_intervention(radius_blue, limit_blue,\n",
    "                        radius_yellow, limit_yellow, reform_year = reform_year)\n",
    "\n",
    "    aggregate = parcel_interventions[['census_tract', 'year', 'intervention']].groupby(['census_tract', 'year']).mean().reset_index()\n",
    "\n",
    "    if not hasattr(tracts_intervention, \"global_census_ids\"):\n",
    "\n",
    "        tracts_intervention.global_valid_pairs = set(zip(values_intervention.global_census_ids['census_tract'],\n",
    "                                     values_intervention.global_census_ids['year']))\n",
    "\n",
    "\n",
    "    subaggregate = aggregate[aggregate[['census_tract', 'year']].apply(tuple, axis=1).isin(tracts_intervention.global_valid_pairs)].copy()\n",
    "\n",
    "    return torch.tensor(list(subaggregate['intervention']))\n",
    "\n",
    "    # plt.hist(aggregate['intervention'])\n",
    "    # plt.show()\n",
    "    #return torch.tensor(subaggregate['intervention'])\n",
    "\n",
    "start = time.time()\n",
    "t_intervention = tracts_intervention(300, .5, 700, .7, reform_year = 2015)\n",
    "end = time.time()\n",
    "print(\"Time to run tracts_intervention 1: \", end - start)\n",
    "\n",
    "start2 = time.time()\n",
    "t_intervention2 = tracts_intervention(400, .5, 800, .6, reform_year = 2013)\n",
    "end2 = time.time()\n",
    "print(\"Time to run tracts_intervention 2: \", end2 - start2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([816])\n",
      "torch.Size([1000, 2, 1, 1, 1, 816])\n",
      "torch.Size([1000, 2, 1, 1, 1, 816])\n"
     ]
    }
   ],
   "source": [
    "print(t_intervention.shape)\n",
    "\n",
    "\n",
    "with MultiWorldCounterfactual() as mwc:\n",
    "    with do(actions={\"limit\": t_intervention}):\n",
    "        samples = predictive(**subset_for_preds)\n",
    "\n",
    "\n",
    "print(samples[\"limit\"].shape)\n",
    "print(samples[\"housing_units\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n",
      "{(0, 0.0, 600, 0.0): 0, (0, 0.0, 600, 0.2): 1, (0, 0.0, 600, 0.4): 2, (0, 0.0, 600, 0.6): 3, (0, 0.0, 600, 0.8): 4, (0, 0.0, 600, 1.0): 5, (0, 0.0, 1200, 0.0): 6, (0, 0.0, 1200, 0.2): 7, (0, 0.0, 1200, 0.4): 8, (0, 0.0, 1200, 0.6): 9, (0, 0.0, 1200, 0.8): 10, (0, 0.0, 1200, 1.0): 11, (0, 0.0, 1800, 0.0): 12, (0, 0.0, 1800, 0.2): 13, (0, 0.0, 1800, 0.4): 14, (0, 0.0, 1800, 0.6): 15, (0, 0.0, 1800, 0.8): 16, (0, 0.0, 1800, 1.0): 17, (0, 0.2, 600, 0.0): 18, (0, 0.2, 600, 0.2): 19, (0, 0.2, 600, 0.4): 20, (0, 0.2, 600, 0.6): 21, (0, 0.2, 600, 0.8): 22, (0, 0.2, 600, 1.0): 23, (0, 0.2, 1200, 0.0): 24, (0, 0.2, 1200, 0.2): 25, (0, 0.2, 1200, 0.4): 26, (0, 0.2, 1200, 0.6): 27, (0, 0.2, 1200, 0.8): 28, (0, 0.2, 1200, 1.0): 29, (0, 0.2, 1800, 0.0): 30, (0, 0.2, 1800, 0.2): 31, (0, 0.2, 1800, 0.4): 32, (0, 0.2, 1800, 0.6): 33, (0, 0.2, 1800, 0.8): 34, (0, 0.2, 1800, 1.0): 35, (0, 0.4, 600, 0.0): 36, (0, 0.4, 600, 0.2): 37, (0, 0.4, 600, 0.4): 38, (0, 0.4, 600, 0.6): 39, (0, 0.4, 600, 0.8): 40, (0, 0.4, 600, 1.0): 41, (0, 0.4, 1200, 0.0): 42, (0, 0.4, 1200, 0.2): 43, (0, 0.4, 1200, 0.4): 44, (0, 0.4, 1200, 0.6): 45, (0, 0.4, 1200, 0.8): 46, (0, 0.4, 1200, 1.0): 47, (0, 0.4, 1800, 0.0): 48, (0, 0.4, 1800, 0.2): 49, (0, 0.4, 1800, 0.4): 50, (0, 0.4, 1800, 0.6): 51, (0, 0.4, 1800, 0.8): 52, (0, 0.4, 1800, 1.0): 53, (0, 0.6, 600, 0.0): 54, (0, 0.6, 600, 0.2): 55, (0, 0.6, 600, 0.4): 56, (0, 0.6, 600, 0.6): 57, (0, 0.6, 600, 0.8): 58, (0, 0.6, 600, 1.0): 59, (0, 0.6, 1200, 0.0): 60, (0, 0.6, 1200, 0.2): 61, (0, 0.6, 1200, 0.4): 62, (0, 0.6, 1200, 0.6): 63, (0, 0.6, 1200, 0.8): 64, (0, 0.6, 1200, 1.0): 65, (0, 0.6, 1800, 0.0): 66, (0, 0.6, 1800, 0.2): 67, (0, 0.6, 1800, 0.4): 68, (0, 0.6, 1800, 0.6): 69, (0, 0.6, 1800, 0.8): 70, (0, 0.6, 1800, 1.0): 71, (0, 0.8, 600, 0.0): 72, (0, 0.8, 600, 0.2): 73, (0, 0.8, 600, 0.4): 74, (0, 0.8, 600, 0.6): 75, (0, 0.8, 600, 0.8): 76, (0, 0.8, 600, 1.0): 77, (0, 0.8, 1200, 0.0): 78, (0, 0.8, 1200, 0.2): 79, (0, 0.8, 1200, 0.4): 80, (0, 0.8, 1200, 0.6): 81, (0, 0.8, 1200, 0.8): 82, (0, 0.8, 1200, 1.0): 83, (0, 0.8, 1800, 0.0): 84, (0, 0.8, 1800, 0.2): 85, (0, 0.8, 1800, 0.4): 86, (0, 0.8, 1800, 0.6): 87, (0, 0.8, 1800, 0.8): 88, (0, 0.8, 1800, 1.0): 89, (0, 1.0, 600, 0.0): 90, (0, 1.0, 600, 0.2): 91, (0, 1.0, 600, 0.4): 92, (0, 1.0, 600, 0.6): 93, (0, 1.0, 600, 0.8): 94, (0, 1.0, 600, 1.0): 95, (0, 1.0, 1200, 0.0): 96, (0, 1.0, 1200, 0.2): 97, (0, 1.0, 1200, 0.4): 98, (0, 1.0, 1200, 0.6): 99, (0, 1.0, 1200, 0.8): 100, (0, 1.0, 1200, 1.0): 101, (0, 1.0, 1800, 0.0): 102, (0, 1.0, 1800, 0.2): 103, (0, 1.0, 1800, 0.4): 104, (0, 1.0, 1800, 0.6): 105, (0, 1.0, 1800, 0.8): 106, (0, 1.0, 1800, 1.0): 107, (600, 0.0, 1200, 0.0): 108, (600, 0.0, 1200, 0.2): 109, (600, 0.0, 1200, 0.4): 110, (600, 0.0, 1200, 0.6): 111, (600, 0.0, 1200, 0.8): 112, (600, 0.0, 1200, 1.0): 113, (600, 0.0, 1800, 0.0): 114, (600, 0.0, 1800, 0.2): 115, (600, 0.0, 1800, 0.4): 116, (600, 0.0, 1800, 0.6): 117, (600, 0.0, 1800, 0.8): 118, (600, 0.0, 1800, 1.0): 119, (600, 0.2, 1200, 0.0): 120, (600, 0.2, 1200, 0.2): 121, (600, 0.2, 1200, 0.4): 122, (600, 0.2, 1200, 0.6): 123, (600, 0.2, 1200, 0.8): 124, (600, 0.2, 1200, 1.0): 125, (600, 0.2, 1800, 0.0): 126, (600, 0.2, 1800, 0.2): 127, (600, 0.2, 1800, 0.4): 128, (600, 0.2, 1800, 0.6): 129, (600, 0.2, 1800, 0.8): 130, (600, 0.2, 1800, 1.0): 131, (600, 0.4, 1200, 0.0): 132, (600, 0.4, 1200, 0.2): 133, (600, 0.4, 1200, 0.4): 134, (600, 0.4, 1200, 0.6): 135, (600, 0.4, 1200, 0.8): 136, (600, 0.4, 1200, 1.0): 137, (600, 0.4, 1800, 0.0): 138, (600, 0.4, 1800, 0.2): 139, (600, 0.4, 1800, 0.4): 140, (600, 0.4, 1800, 0.6): 141, (600, 0.4, 1800, 0.8): 142, (600, 0.4, 1800, 1.0): 143, (600, 0.6, 1200, 0.0): 144, (600, 0.6, 1200, 0.2): 145, (600, 0.6, 1200, 0.4): 146, (600, 0.6, 1200, 0.6): 147, (600, 0.6, 1200, 0.8): 148, (600, 0.6, 1200, 1.0): 149, (600, 0.6, 1800, 0.0): 150, (600, 0.6, 1800, 0.2): 151, (600, 0.6, 1800, 0.4): 152, (600, 0.6, 1800, 0.6): 153, (600, 0.6, 1800, 0.8): 154, (600, 0.6, 1800, 1.0): 155, (600, 0.8, 1200, 0.0): 156, (600, 0.8, 1200, 0.2): 157, (600, 0.8, 1200, 0.4): 158, (600, 0.8, 1200, 0.6): 159, (600, 0.8, 1200, 0.8): 160, (600, 0.8, 1200, 1.0): 161, (600, 0.8, 1800, 0.0): 162, (600, 0.8, 1800, 0.2): 163, (600, 0.8, 1800, 0.4): 164, (600, 0.8, 1800, 0.6): 165, (600, 0.8, 1800, 0.8): 166, (600, 0.8, 1800, 1.0): 167, (600, 1.0, 1200, 0.0): 168, (600, 1.0, 1200, 0.2): 169, (600, 1.0, 1200, 0.4): 170, (600, 1.0, 1200, 0.6): 171, (600, 1.0, 1200, 0.8): 172, (600, 1.0, 1200, 1.0): 173, (600, 1.0, 1800, 0.0): 174, (600, 1.0, 1800, 0.2): 175, (600, 1.0, 1800, 0.4): 176, (600, 1.0, 1800, 0.6): 177, (600, 1.0, 1800, 0.8): 178, (600, 1.0, 1800, 1.0): 179, (1200, 0.0, 1800, 0.0): 180, (1200, 0.0, 1800, 0.2): 181, (1200, 0.0, 1800, 0.4): 182, (1200, 0.0, 1800, 0.6): 183, (1200, 0.0, 1800, 0.8): 184, (1200, 0.0, 1800, 1.0): 185, (1200, 0.2, 1800, 0.0): 186, (1200, 0.2, 1800, 0.2): 187, (1200, 0.2, 1800, 0.4): 188, (1200, 0.2, 1800, 0.6): 189, (1200, 0.2, 1800, 0.8): 190, (1200, 0.2, 1800, 1.0): 191, (1200, 0.4, 1800, 0.0): 192, (1200, 0.4, 1800, 0.2): 193, (1200, 0.4, 1800, 0.4): 194, (1200, 0.4, 1800, 0.6): 195, (1200, 0.4, 1800, 0.8): 196, (1200, 0.4, 1800, 1.0): 197, (1200, 0.6, 1800, 0.0): 198, (1200, 0.6, 1800, 0.2): 199, (1200, 0.6, 1800, 0.4): 200, (1200, 0.6, 1800, 0.6): 201, (1200, 0.6, 1800, 0.8): 202, (1200, 0.6, 1800, 1.0): 203, (1200, 0.8, 1800, 0.0): 204, (1200, 0.8, 1800, 0.2): 205, (1200, 0.8, 1800, 0.4): 206, (1200, 0.8, 1800, 0.6): 207, (1200, 0.8, 1800, 0.8): 208, (1200, 0.8, 1800, 1.0): 209, (1200, 1.0, 1800, 0.0): 210, (1200, 1.0, 1800, 0.2): 211, (1200, 1.0, 1800, 0.4): 212, (1200, 1.0, 1800, 0.6): 213, (1200, 1.0, 1800, 0.8): 214, (1200, 1.0, 1800, 1.0): 215}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_intervention_settings(\n",
    "    distance_interval=50, distance_max=2500, limit_interval=0.2\n",
    "):\n",
    "    \n",
    "    dist = np.arange(0, distance_max, distance_interval)\n",
    "    dist = dist.round()  # to avoid small numerical issues\n",
    "\n",
    "    lim = np.arange(0, 1.01, limit_interval)\n",
    "    lim = lim.round(decimals=1)\n",
    "    n_lim = len(lim)\n",
    "\n",
    "    counter = 0            \n",
    "    interventions_settings_dict = {}\n",
    "    for d_blue in dist:\n",
    "        for l_blue in lim:\n",
    "            for d_yellow in dist[dist > d_blue]:  # note the built in restriction\n",
    "                for l_yellow in lim:\n",
    "                    \n",
    "                    interventions_settings_dict[(d_blue, l_blue, d_yellow, l_yellow)] = (\n",
    "                        counter\n",
    "                    )\n",
    "                    counter += 1\n",
    "\n",
    "    return interventions_settings_dict\n",
    "\n",
    "interventions_settings_dict = generate_intervention_settings(distance_interval=600, distance_max=2000, limit_interval=0.2)\n",
    "\n",
    "print(len(interventions_settings_dict.keys()))\n",
    "\n",
    "print(interventions_settings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing dictionary with 216 entries\n",
      "Time of last run:  0.06448507308959961 ,  0 vectors computed.\n",
      "216 exist out of 216 desired (ratio: 1.0)\n"
     ]
    }
   ],
   "source": [
    "def generate_interventions(interventions_settings_dict):\n",
    "   \n",
    "\n",
    "    interventions_path = os.path.join(root, \"data/minneapolis/processed/tract_interventions_tuple.pkl\")\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "    if os.path.exists(interventions_path):\n",
    "            with open(interventions_path, \"rb\") as file:\n",
    "                intervention_vectors_dict = dill.load(file)\n",
    "            print(f\"Loaded existing dictionary with {len(intervention_vectors_dict)} entries\")\n",
    "    else:\n",
    "        print(\"No existing dictionary found, creating new one\")\n",
    "        intervention_vectors_dict = {}\n",
    "\n",
    "    counter = 0 \n",
    "    for key in interventions_settings_dict.keys():\n",
    "        \n",
    "\n",
    "        if key not in intervention_vectors_dict:\n",
    "            intervention_vectors_dict[key] = tracts_intervention(\n",
    "                radius_blue=key[0],\n",
    "                limit_blue=key[1],\n",
    "                radius_yellow=key[2],\n",
    "                limit_yellow=key[3],\n",
    "            )\n",
    "\n",
    "            counter += 1\n",
    "            if counter % 100 == 0:\n",
    "                    print(f\"Saving {key} at step {counter}\")\n",
    "                    with open(interventions_path, \"wb\") as file:\n",
    "                        dill.dump(intervention_vectors_dict, file)\n",
    "                    print(f\"{len(intervention_vectors_dict)} exist out of {len(interventions_settings_dict)} desired (ratio: {len(intervention_vectors_dict)/len(interventions_settings_dict)})\")\n",
    "\n",
    "\n",
    "    with open(interventions_path, \"wb\") as file:\n",
    "        dill.dump(intervention_vectors_dict, file)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(\"Time of last run: \", end - start, \", \", counter, \"vectors computed.\")\n",
    "    print(f\"{len(intervention_vectors_dict)} exist out of {len(interventions_settings_dict)} desired (ratio: {len(intervention_vectors_dict)/len(interventions_settings_dict)})\")\n",
    "\n",
    "    return intervention_vectors_dict\n",
    "\n",
    "intervention_vectors_dict = generate_interventions(interventions_settings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded existing dictionary with 217 entries\n",
      "Skipping batch 1 out of 3 as computed.\n",
      "Skipping batch 2 out of 3 as computed.\n",
      "Skipping batch 3 out of 3 as computed.\n"
     ]
    }
   ],
   "source": [
    "def generate_intervened_preds(intervention_vectors_dict, \n",
    "                              subset_for_preds, predictive, batch_size=100):\n",
    "\n",
    "    tracts_prediction_path = os.path.join(root, \"data/minneapolis/processed/tract_intervened_predictions.pkl\")\n",
    "\n",
    "    if os.path.exists(tracts_prediction_path):\n",
    "        with open(tracts_prediction_path, \"rb\") as file:\n",
    "            all_preds = dill.load(file)\n",
    "        print(f\"Loaded existing dictionary with {len(all_preds)} entries\")\n",
    "\n",
    "    else:\n",
    "        print(\"No existing dictionary found, creating new one\")\n",
    "        all_preds = {}\n",
    "\n",
    "\n",
    "    keys = list(intervention_vectors_dict.keys())\n",
    "    total_batches = (len(keys) // batch_size) + 1\n",
    "\n",
    "    \n",
    "    batched_keys = {}\n",
    "    batched_samples = {}\n",
    "    mwcs = {}\n",
    "\n",
    "\n",
    "    for batch_idx in range(total_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(keys))\n",
    "\n",
    "        batched_keys[batch_idx] = keys[start_idx:end_idx]\n",
    "        \n",
    "        if all(key in all_preds for key in batched_keys[batch_idx]):\n",
    "            print(f\"Skipping batch {batch_idx + 1} out of {total_batches} as computed.\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        interventions_tuple = tuple(intervention_vectors_dict[key] for key in batched_keys[batch_idx])\n",
    "        print(len(interventions_tuple))\n",
    "\n",
    "        with MultiWorldCounterfactual() as mwc:\n",
    "            with do(actions={\"limit\": interventions_tuple}):\n",
    "                batched_samples[batch_idx] = predictive(**subset_for_preds)\n",
    "        mwcs[batch_idx] = mwc\n",
    "\n",
    "        value = batched_samples[batch_idx]['housing_units']\n",
    "\n",
    "        with mwcs[batch_idx]:\n",
    "            if batch_idx == 0:\n",
    "                all_preds['factual_preds'] = (\n",
    "                    gather(value, IndexSet(**{\"limit\": {0}}), event_dims=0)\n",
    "                    .squeeze()\n",
    "                    .detach()\n",
    "                    .mean(axis=0)\n",
    "                    .numpy()\n",
    "                )\n",
    "\n",
    "\n",
    "            for idx, key in enumerate(batched_keys[batch_idx]):\n",
    "\n",
    "                all_preds[key] = gather(value,\n",
    "                    IndexSet(**{\"limit\": {idx+1}}), event_dims=0).squeeze().detach().mean(axis=0).numpy()\n",
    "            \n",
    "        print(f\"Batch {batch_idx + 1} out of {total_batches} done, saving progress\")\n",
    "        with open(tracts_prediction_path, \"wb\") as file:\n",
    "            dill.dump(all_preds, file)\n",
    "\n",
    "\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "                    \n",
    "all_preds = generate_intervened_preds(intervention_vectors_dict, subset_for_preds, predictive, batch_size=100)\n",
    "\n",
    "\n",
    "assert len(all_preds) == len(intervention_vectors_dict) + 1\n",
    "assert 'factual_preds' in all_preds\n",
    "assert all(all_preds[key].shape == (816,) for key in intervention_vectors_dict.keys()), \\\n",
    "    \"Not all entries in all_preds have the shape (816,)\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chirho",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
