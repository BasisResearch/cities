{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from cities.utils.cleaning_utils import find_repo_root\n",
    "\n",
    "root = find_repo_root()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from cities.utils.cleaning_utils import standardize_and_scale\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from cities.utils.data_grabber import DataGrabber\n",
    "data = DataGrabber()\n",
    "data.get_features_wide([\"gdp\", \"population\", \"transport\",\n",
    "            \"spending_transportation\", \"spending_commerce\", \"spending_HHS\"])\n",
    "gdp = data.wide['gdp']\n",
    "# population = data.wide['population']\n",
    "# trnasport = data.wide['transport']\n",
    "# spending_transportation = data.wide['spending_transportation']\n",
    "# spending_commerce = data.wide['spending_commerce']\n",
    "\n",
    "def tableInfo(tableName):\n",
    "\n",
    "    print(tableName.head())\n",
    "    print(tableName.dtypes)\n",
    "    print(f'Number of rows: {tableName.shape[0]}')\n",
    "    print(f'Unique FIPS numbers {tableName['GeoFIPS'].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique metropolitan areas 384\n",
      "Number of unique Counties 1160\n"
     ]
    }
   ],
   "source": [
    "column_names = ['col1', 'metro_area', 'col3', 'GeoName']\n",
    "\n",
    "metro_areas = pd.read_csv(f\"{root}/data/raw/metrolist.csv\", header=None, names=column_names)\n",
    "\n",
    "\n",
    "\n",
    "unique_metro_areas = metro_areas['metro_area'].nunique()\n",
    "print(f\"Number of unique metropolitan areas {unique_metro_areas}\")\n",
    "\n",
    "unique_geo_names = metro_areas['GeoName'].nunique()\n",
    "print(f\"Number of unique Counties {unique_geo_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>CountyFIPS</th>\n",
       "      <th>CountyName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>48059</td>\n",
       "      <td>Callahan, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>48253</td>\n",
       "      <td>Jones, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>48441</td>\n",
       "      <td>Taylor, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10420</td>\n",
       "      <td>Akron, OH (MA)</td>\n",
       "      <td>39133</td>\n",
       "      <td>Portage, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10420</td>\n",
       "      <td>Akron, OH (MA)</td>\n",
       "      <td>39153</td>\n",
       "      <td>Summit, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>49660</td>\n",
       "      <td>Youngstown-Warren-Boardman, OH-PA (MA)</td>\n",
       "      <td>39155</td>\n",
       "      <td>Trumbull, OH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>49660</td>\n",
       "      <td>Youngstown-Warren-Boardman, OH-PA (MA)</td>\n",
       "      <td>42085</td>\n",
       "      <td>Mercer, PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>49700</td>\n",
       "      <td>Yuba City, CA (MA)</td>\n",
       "      <td>6101</td>\n",
       "      <td>Sutter, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>49700</td>\n",
       "      <td>Yuba City, CA (MA)</td>\n",
       "      <td>6115</td>\n",
       "      <td>Yuba, CA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>4027</td>\n",
       "      <td>Yuma, AZ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1160 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      GeoFIPS                                 GeoName  CountyFIPS  \\\n",
       "0       10180                        Abilene, TX (MA)       48059   \n",
       "1       10180                        Abilene, TX (MA)       48253   \n",
       "2       10180                        Abilene, TX (MA)       48441   \n",
       "3       10420                          Akron, OH (MA)       39133   \n",
       "4       10420                          Akron, OH (MA)       39153   \n",
       "...       ...                                     ...         ...   \n",
       "1155    49660  Youngstown-Warren-Boardman, OH-PA (MA)       39155   \n",
       "1156    49660  Youngstown-Warren-Boardman, OH-PA (MA)       42085   \n",
       "1157    49700                      Yuba City, CA (MA)        6101   \n",
       "1158    49700                      Yuba City, CA (MA)        6115   \n",
       "1159    49740                           Yuma, AZ (MA)        4027   \n",
       "\n",
       "        CountyName  \n",
       "0     Callahan, TX  \n",
       "1        Jones, TX  \n",
       "2       Taylor, TX  \n",
       "3      Portage, OH  \n",
       "4       Summit, OH  \n",
       "...            ...  \n",
       "1155  Trumbull, OH  \n",
       "1156    Mercer, PA  \n",
       "1157    Sutter, CA  \n",
       "1158      Yuba, CA  \n",
       "1159      Yuma, AZ  \n",
       "\n",
       "[1160 rows x 4 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's treat as a source of truth for metro areas\n",
    "\n",
    "metro_areas = metro_areas.rename(columns={'GeoName': 'CountyName', 'col3': 'CountyFIPS', 'col1': 'GeoFIPS',\n",
    "                                              'metro_area': 'GeoName'})\n",
    "metro_areas = metro_areas.sort_values(by=['GeoFIPS', 'CountyFIPS']).reset_index(drop=True)\n",
    "\n",
    "metro_areas['GeoName'] = metro_areas['GeoName'].astype(str)\n",
    "\n",
    "metro_areas['GeoName'] = metro_areas['GeoName'].str.extract(r'^(.*?)\\s*\\(', expand=False).fillna('')\n",
    "metro_areas['GeoName'] = metro_areas['GeoName'] + ' (MA)'\n",
    "metro_areas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare geonames with cities source of truthe\n",
    "\n",
    "\n",
    "\n",
    "# GeoNames that are not common with GDP\n",
    "geo_names_not_in_gdp = metro_areas[~metro_areas['GeoName'].isin(gdp['GeoName'])]\n",
    "geo_names_not_in_gdp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "api_key = \"ED8C2AB7-DD09-4EE0-AC91-116A9E05348A\"\n",
    "\n",
    "\n",
    "years = '1993,1994,1995,1996,1997,1998,1999,2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017,2018,2019,2020,2021'\n",
    "\n",
    "url = f\"https://apps.bea.gov/api/data/?UserID={api_key}&method=GetData&datasetname=Regional&TableName=CAINC30&LineCode=100&Year={years}&GeoFips=MSA&ResultFormat=json\"\n",
    "response = requests.get(url)\n",
    "assert response.status_code == 200\n",
    "\n",
    "# 100 stands for population\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "\n",
    "data_series = data['BEAAPI']['Results']['Data']\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data_series)\n",
    "\n",
    "\n",
    "# metro_areas fips rules: always 5 digits, last is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>GeoFips</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>TimePeriod</th>\n",
       "      <th>CL_UNIT</th>\n",
       "      <th>UNIT_MULT</th>\n",
       "      <th>DataValue</th>\n",
       "      <th>NoteRef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CAINC30-100</td>\n",
       "      <td>00998</td>\n",
       "      <td>United States (Metropolitan Portion)</td>\n",
       "      <td>1997</td>\n",
       "      <td>Number of persons</td>\n",
       "      <td>0</td>\n",
       "      <td>229081203</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CAINC30-100</td>\n",
       "      <td>00998</td>\n",
       "      <td>United States (Metropolitan Portion)</td>\n",
       "      <td>2005</td>\n",
       "      <td>Number of persons</td>\n",
       "      <td>0</td>\n",
       "      <td>250646689</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CAINC30-100</td>\n",
       "      <td>00998</td>\n",
       "      <td>United States (Metropolitan Portion)</td>\n",
       "      <td>2009</td>\n",
       "      <td>Number of persons</td>\n",
       "      <td>0</td>\n",
       "      <td>261210075</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAINC30-100</td>\n",
       "      <td>00998</td>\n",
       "      <td>United States (Metropolitan Portion)</td>\n",
       "      <td>2018</td>\n",
       "      <td>Number of persons</td>\n",
       "      <td>0</td>\n",
       "      <td>283180665</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CAINC30-100</td>\n",
       "      <td>00998</td>\n",
       "      <td>United States (Metropolitan Portion)</td>\n",
       "      <td>2001</td>\n",
       "      <td>Number of persons</td>\n",
       "      <td>0</td>\n",
       "      <td>240717866</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Code GeoFips                               GeoName TimePeriod  \\\n",
       "0  CAINC30-100   00998  United States (Metropolitan Portion)       1997   \n",
       "1  CAINC30-100   00998  United States (Metropolitan Portion)       2005   \n",
       "2  CAINC30-100   00998  United States (Metropolitan Portion)       2009   \n",
       "3  CAINC30-100   00998  United States (Metropolitan Portion)       2018   \n",
       "4  CAINC30-100   00998  United States (Metropolitan Portion)       2001   \n",
       "\n",
       "             CL_UNIT UNIT_MULT  DataValue NoteRef  \n",
       "0  Number of persons         0  229081203       3  \n",
       "1  Number of persons         0  250646689       3  \n",
       "2  Number of persons         0  261210075       3  \n",
       "3  Number of persons         0  283180665       3  \n",
       "4  Number of persons         0  240717866       3  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (Metropolitan Statistical Area)</td>\n",
       "      <td>1993</td>\n",
       "      <td>152909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (Metropolitan Statistical Area)</td>\n",
       "      <td>1994</td>\n",
       "      <td>153779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (Metropolitan Statistical Area)</td>\n",
       "      <td>1995</td>\n",
       "      <td>156097.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (Metropolitan Statistical Area)</td>\n",
       "      <td>1996</td>\n",
       "      <td>156351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (Metropolitan Statistical Area)</td>\n",
       "      <td>1997</td>\n",
       "      <td>157405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11131</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (Metropolitan Statistical Area) *</td>\n",
       "      <td>2017</td>\n",
       "      <td>199915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11132</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (Metropolitan Statistical Area) *</td>\n",
       "      <td>2018</td>\n",
       "      <td>200572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11133</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (Metropolitan Statistical Area) *</td>\n",
       "      <td>2019</td>\n",
       "      <td>202099.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11134</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (Metropolitan Statistical Area) *</td>\n",
       "      <td>2020</td>\n",
       "      <td>204528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11135</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (Metropolitan Statistical Area) *</td>\n",
       "      <td>2021</td>\n",
       "      <td>206241.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11136 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GeoFIPS                                      GeoName  Year     Value\n",
       "0        10180  Abilene, TX (Metropolitan Statistical Area)  1993  152909.0\n",
       "1        10180  Abilene, TX (Metropolitan Statistical Area)  1994  153779.0\n",
       "2        10180  Abilene, TX (Metropolitan Statistical Area)  1995  156097.0\n",
       "3        10180  Abilene, TX (Metropolitan Statistical Area)  1996  156351.0\n",
       "4        10180  Abilene, TX (Metropolitan Statistical Area)  1997  157405.0\n",
       "...        ...                                          ...   ...       ...\n",
       "11131    49740   Yuma, AZ (Metropolitan Statistical Area) *  2017  199915.0\n",
       "11132    49740   Yuma, AZ (Metropolitan Statistical Area) *  2018  200572.0\n",
       "11133    49740   Yuma, AZ (Metropolitan Statistical Area) *  2019  202099.0\n",
       "11134    49740   Yuma, AZ (Metropolitan Statistical Area) *  2020  204528.0\n",
       "11135    49740   Yuma, AZ (Metropolitan Statistical Area) *  2021  206241.0\n",
       "\n",
       "[11136 rows x 4 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_ma = df.copy()\n",
    "\n",
    "population_ma = population_ma[population_ma['GeoFips'].str.endswith('0')]\n",
    "\n",
    "\n",
    "population_ma = population_ma[['GeoFips', 'GeoName', 'TimePeriod', 'DataValue']]\n",
    "population_ma = population_ma.rename(columns={'DataValue': 'Value', 'TimePeriod': 'Year', 'GeoFips': 'GeoFIPS' })\n",
    "population_ma = population_ma.sort_values(by=['GeoFIPS', 'Year']).reset_index(drop=True)\n",
    "population_ma['Value'] = population_ma['Value'].astype(float)\n",
    "population_ma['GeoName'] = population_ma['GeoName'].astype(str)\n",
    "population_ma['GeoFIPS'] = population_ma['GeoFIPS'].astype(np.int64)\n",
    "\n",
    "population_ma\n",
    "\n",
    "# MA is a standard metropolitan statistical area abbreaviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>1993</td>\n",
       "      <td>152909.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>1994</td>\n",
       "      <td>153779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>1995</td>\n",
       "      <td>156097.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>1996</td>\n",
       "      <td>156351.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>1997</td>\n",
       "      <td>157405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11131</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>2017</td>\n",
       "      <td>199915.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11132</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>2018</td>\n",
       "      <td>200572.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11133</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>2019</td>\n",
       "      <td>202099.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11134</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>2020</td>\n",
       "      <td>204528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11135</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>2021</td>\n",
       "      <td>206241.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11136 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       GeoFIPS           GeoName  Year     Value\n",
       "0        10180  Abilene, TX (MA)  1993  152909.0\n",
       "1        10180  Abilene, TX (MA)  1994  153779.0\n",
       "2        10180  Abilene, TX (MA)  1995  156097.0\n",
       "3        10180  Abilene, TX (MA)  1996  156351.0\n",
       "4        10180  Abilene, TX (MA)  1997  157405.0\n",
       "...        ...               ...   ...       ...\n",
       "11131    49740     Yuma, AZ (MA)  2017  199915.0\n",
       "11132    49740     Yuma, AZ (MA)  2018  200572.0\n",
       "11133    49740     Yuma, AZ (MA)  2019  202099.0\n",
       "11134    49740     Yuma, AZ (MA)  2020  204528.0\n",
       "11135    49740     Yuma, AZ (MA)  2021  206241.0\n",
       "\n",
       "[11136 rows x 4 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_ma['GeoName'] = population_ma['GeoName'].str.extract(r'^(.*?)\\s*\\(', expand=False).fillna('')\n",
    "population_ma['GeoName'] = population_ma['GeoName'] + ' (MA)'\n",
    "\n",
    "population_ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>...</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>2018</th>\n",
       "      <th>2019</th>\n",
       "      <th>2020</th>\n",
       "      <th>2021</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10180</td>\n",
       "      <td>Abilene, TX (MA)</td>\n",
       "      <td>152909.0</td>\n",
       "      <td>153779.0</td>\n",
       "      <td>156097.0</td>\n",
       "      <td>156351.0</td>\n",
       "      <td>157405.0</td>\n",
       "      <td>158264.0</td>\n",
       "      <td>159755.0</td>\n",
       "      <td>160288.0</td>\n",
       "      <td>...</td>\n",
       "      <td>168246.0</td>\n",
       "      <td>168614.0</td>\n",
       "      <td>169859.0</td>\n",
       "      <td>171579.0</td>\n",
       "      <td>172242.0</td>\n",
       "      <td>172915.0</td>\n",
       "      <td>174005.0</td>\n",
       "      <td>175187.0</td>\n",
       "      <td>176866.0</td>\n",
       "      <td>177829.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10420</td>\n",
       "      <td>Akron, OH (MA)</td>\n",
       "      <td>674114.0</td>\n",
       "      <td>678063.0</td>\n",
       "      <td>682146.0</td>\n",
       "      <td>687264.0</td>\n",
       "      <td>689461.0</td>\n",
       "      <td>691039.0</td>\n",
       "      <td>693125.0</td>\n",
       "      <td>695946.0</td>\n",
       "      <td>...</td>\n",
       "      <td>702245.0</td>\n",
       "      <td>703809.0</td>\n",
       "      <td>705134.0</td>\n",
       "      <td>704664.0</td>\n",
       "      <td>703862.0</td>\n",
       "      <td>704229.0</td>\n",
       "      <td>703925.0</td>\n",
       "      <td>703361.0</td>\n",
       "      <td>701625.0</td>\n",
       "      <td>696225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10500</td>\n",
       "      <td>Albany, GA (MA)</td>\n",
       "      <td>148122.0</td>\n",
       "      <td>149356.0</td>\n",
       "      <td>150100.0</td>\n",
       "      <td>151596.0</td>\n",
       "      <td>152692.0</td>\n",
       "      <td>153449.0</td>\n",
       "      <td>153555.0</td>\n",
       "      <td>153585.0</td>\n",
       "      <td>...</td>\n",
       "      <td>154669.0</td>\n",
       "      <td>153680.0</td>\n",
       "      <td>153263.0</td>\n",
       "      <td>152018.0</td>\n",
       "      <td>151044.0</td>\n",
       "      <td>150387.0</td>\n",
       "      <td>150440.0</td>\n",
       "      <td>149530.0</td>\n",
       "      <td>148244.0</td>\n",
       "      <td>144922.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10540</td>\n",
       "      <td>Albany-Lebanon, OR (MA)</td>\n",
       "      <td>95496.0</td>\n",
       "      <td>96919.0</td>\n",
       "      <td>98853.0</td>\n",
       "      <td>100582.0</td>\n",
       "      <td>102054.0</td>\n",
       "      <td>102770.0</td>\n",
       "      <td>103462.0</td>\n",
       "      <td>103020.0</td>\n",
       "      <td>...</td>\n",
       "      <td>117865.0</td>\n",
       "      <td>117822.0</td>\n",
       "      <td>118285.0</td>\n",
       "      <td>119295.0</td>\n",
       "      <td>121648.0</td>\n",
       "      <td>123657.0</td>\n",
       "      <td>125788.0</td>\n",
       "      <td>127700.0</td>\n",
       "      <td>128978.0</td>\n",
       "      <td>129948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10580</td>\n",
       "      <td>Albany-Schenectady-Troy, NY (MA)</td>\n",
       "      <td>827812.0</td>\n",
       "      <td>830817.0</td>\n",
       "      <td>830439.0</td>\n",
       "      <td>828007.0</td>\n",
       "      <td>824711.0</td>\n",
       "      <td>823712.0</td>\n",
       "      <td>824119.0</td>\n",
       "      <td>827399.0</td>\n",
       "      <td>...</td>\n",
       "      <td>879397.0</td>\n",
       "      <td>883854.0</td>\n",
       "      <td>886981.0</td>\n",
       "      <td>889918.0</td>\n",
       "      <td>892762.0</td>\n",
       "      <td>897172.0</td>\n",
       "      <td>899605.0</td>\n",
       "      <td>898791.0</td>\n",
       "      <td>899748.0</td>\n",
       "      <td>905369.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>49420</td>\n",
       "      <td>Yakima, WA (MA)</td>\n",
       "      <td>204266.0</td>\n",
       "      <td>208963.0</td>\n",
       "      <td>212601.0</td>\n",
       "      <td>214951.0</td>\n",
       "      <td>217201.0</td>\n",
       "      <td>219748.0</td>\n",
       "      <td>221573.0</td>\n",
       "      <td>222615.0</td>\n",
       "      <td>...</td>\n",
       "      <td>247072.0</td>\n",
       "      <td>247853.0</td>\n",
       "      <td>248660.0</td>\n",
       "      <td>250020.0</td>\n",
       "      <td>252180.0</td>\n",
       "      <td>253166.0</td>\n",
       "      <td>254420.0</td>\n",
       "      <td>255815.0</td>\n",
       "      <td>256702.0</td>\n",
       "      <td>256647.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>49620</td>\n",
       "      <td>York-Hanover, PA (MA)</td>\n",
       "      <td>356156.0</td>\n",
       "      <td>361092.0</td>\n",
       "      <td>365997.0</td>\n",
       "      <td>369781.0</td>\n",
       "      <td>372706.0</td>\n",
       "      <td>375810.0</td>\n",
       "      <td>378905.0</td>\n",
       "      <td>382743.0</td>\n",
       "      <td>...</td>\n",
       "      <td>438872.0</td>\n",
       "      <td>440954.0</td>\n",
       "      <td>443127.0</td>\n",
       "      <td>445022.0</td>\n",
       "      <td>447735.0</td>\n",
       "      <td>450261.0</td>\n",
       "      <td>453380.0</td>\n",
       "      <td>454912.0</td>\n",
       "      <td>456692.0</td>\n",
       "      <td>459148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>49660</td>\n",
       "      <td>Youngstown-Warren-Boardman, OH-PA (MA)</td>\n",
       "      <td>618328.0</td>\n",
       "      <td>617253.0</td>\n",
       "      <td>615595.0</td>\n",
       "      <td>614369.0</td>\n",
       "      <td>611902.0</td>\n",
       "      <td>609286.0</td>\n",
       "      <td>605978.0</td>\n",
       "      <td>602227.0</td>\n",
       "      <td>...</td>\n",
       "      <td>560540.0</td>\n",
       "      <td>558798.0</td>\n",
       "      <td>556915.0</td>\n",
       "      <td>553591.0</td>\n",
       "      <td>550063.0</td>\n",
       "      <td>547497.0</td>\n",
       "      <td>545030.0</td>\n",
       "      <td>542582.0</td>\n",
       "      <td>540211.0</td>\n",
       "      <td>537837.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>49700</td>\n",
       "      <td>Yuba City, CA (MA)</td>\n",
       "      <td>131950.0</td>\n",
       "      <td>134518.0</td>\n",
       "      <td>135323.0</td>\n",
       "      <td>136160.0</td>\n",
       "      <td>136425.0</td>\n",
       "      <td>137016.0</td>\n",
       "      <td>138097.0</td>\n",
       "      <td>139564.0</td>\n",
       "      <td>...</td>\n",
       "      <td>167918.0</td>\n",
       "      <td>168982.0</td>\n",
       "      <td>170146.0</td>\n",
       "      <td>171626.0</td>\n",
       "      <td>173529.0</td>\n",
       "      <td>176069.0</td>\n",
       "      <td>177586.0</td>\n",
       "      <td>179785.0</td>\n",
       "      <td>181458.0</td>\n",
       "      <td>182254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>49740</td>\n",
       "      <td>Yuma, AZ (MA)</td>\n",
       "      <td>124892.0</td>\n",
       "      <td>127975.0</td>\n",
       "      <td>131776.0</td>\n",
       "      <td>137248.0</td>\n",
       "      <td>143896.0</td>\n",
       "      <td>149065.0</td>\n",
       "      <td>155665.0</td>\n",
       "      <td>160576.0</td>\n",
       "      <td>...</td>\n",
       "      <td>199666.0</td>\n",
       "      <td>198339.0</td>\n",
       "      <td>198670.0</td>\n",
       "      <td>198510.0</td>\n",
       "      <td>199165.0</td>\n",
       "      <td>199915.0</td>\n",
       "      <td>200572.0</td>\n",
       "      <td>202099.0</td>\n",
       "      <td>204528.0</td>\n",
       "      <td>206241.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>384 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GeoFIPS                                 GeoName      1993      1994  \\\n",
       "0      10180                        Abilene, TX (MA)  152909.0  153779.0   \n",
       "1      10420                          Akron, OH (MA)  674114.0  678063.0   \n",
       "2      10500                         Albany, GA (MA)  148122.0  149356.0   \n",
       "3      10540                 Albany-Lebanon, OR (MA)   95496.0   96919.0   \n",
       "4      10580        Albany-Schenectady-Troy, NY (MA)  827812.0  830817.0   \n",
       "..       ...                                     ...       ...       ...   \n",
       "379    49420                         Yakima, WA (MA)  204266.0  208963.0   \n",
       "380    49620                   York-Hanover, PA (MA)  356156.0  361092.0   \n",
       "381    49660  Youngstown-Warren-Boardman, OH-PA (MA)  618328.0  617253.0   \n",
       "382    49700                      Yuba City, CA (MA)  131950.0  134518.0   \n",
       "383    49740                           Yuma, AZ (MA)  124892.0  127975.0   \n",
       "\n",
       "         1995      1996      1997      1998      1999      2000  ...  \\\n",
       "0    156097.0  156351.0  157405.0  158264.0  159755.0  160288.0  ...   \n",
       "1    682146.0  687264.0  689461.0  691039.0  693125.0  695946.0  ...   \n",
       "2    150100.0  151596.0  152692.0  153449.0  153555.0  153585.0  ...   \n",
       "3     98853.0  100582.0  102054.0  102770.0  103462.0  103020.0  ...   \n",
       "4    830439.0  828007.0  824711.0  823712.0  824119.0  827399.0  ...   \n",
       "..        ...       ...       ...       ...       ...       ...  ...   \n",
       "379  212601.0  214951.0  217201.0  219748.0  221573.0  222615.0  ...   \n",
       "380  365997.0  369781.0  372706.0  375810.0  378905.0  382743.0  ...   \n",
       "381  615595.0  614369.0  611902.0  609286.0  605978.0  602227.0  ...   \n",
       "382  135323.0  136160.0  136425.0  137016.0  138097.0  139564.0  ...   \n",
       "383  131776.0  137248.0  143896.0  149065.0  155665.0  160576.0  ...   \n",
       "\n",
       "         2012      2013      2014      2015      2016      2017      2018  \\\n",
       "0    168246.0  168614.0  169859.0  171579.0  172242.0  172915.0  174005.0   \n",
       "1    702245.0  703809.0  705134.0  704664.0  703862.0  704229.0  703925.0   \n",
       "2    154669.0  153680.0  153263.0  152018.0  151044.0  150387.0  150440.0   \n",
       "3    117865.0  117822.0  118285.0  119295.0  121648.0  123657.0  125788.0   \n",
       "4    879397.0  883854.0  886981.0  889918.0  892762.0  897172.0  899605.0   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "379  247072.0  247853.0  248660.0  250020.0  252180.0  253166.0  254420.0   \n",
       "380  438872.0  440954.0  443127.0  445022.0  447735.0  450261.0  453380.0   \n",
       "381  560540.0  558798.0  556915.0  553591.0  550063.0  547497.0  545030.0   \n",
       "382  167918.0  168982.0  170146.0  171626.0  173529.0  176069.0  177586.0   \n",
       "383  199666.0  198339.0  198670.0  198510.0  199165.0  199915.0  200572.0   \n",
       "\n",
       "         2019      2020      2021  \n",
       "0    175187.0  176866.0  177829.0  \n",
       "1    703361.0  701625.0  696225.0  \n",
       "2    149530.0  148244.0  144922.0  \n",
       "3    127700.0  128978.0  129948.0  \n",
       "4    898791.0  899748.0  905369.0  \n",
       "..        ...       ...       ...  \n",
       "379  255815.0  256702.0  256647.0  \n",
       "380  454912.0  456692.0  459148.0  \n",
       "381  542582.0  540211.0  537837.0  \n",
       "382  179785.0  181458.0  182254.0  \n",
       "383  202099.0  204528.0  206241.0  \n",
       "\n",
       "[384 rows x 31 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_wide = population_ma.pivot(index=['GeoFIPS', 'GeoName'], columns='Year', values='Value')\n",
    "population_wide = population_wide.reset_index()\n",
    "population_wide.columns.name = None\n",
    "population_wide\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = len(population_ma)\n",
    "unique_years = population_ma['Year'].nunique()\n",
    "unique_geo_fips = population_ma['GeoFIPS'].nunique()\n",
    "\n",
    "assert (unique_years * unique_geo_fips) == total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "population_ma['GeoFIPS'].nunique() == metro_areas['GeoFIPS'].nunique() # 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GeoFIPS</th>\n",
       "      <th>GeoName</th>\n",
       "      <th>Year</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [GeoFIPS, GeoName, Year, Value]\n",
       "Index: []"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ma_not_in_population = population_ma[~population_ma['GeoFIPS'].isin(metro_areas['GeoFIPS'])]\n",
    "ma_not_in_population\n",
    "# should be empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_variable(variable_name, path_to_raw_csv, YearOrCategory=\"Year\", region_type: str = \"county\"):\n",
    "    # function for cleaning a generic timeseries csv, wide format with these columns:\n",
    "    # GeoFIPS, GeoName, 2001, 2002, 2003, 2004, 2005, 2006, 2007, ...\n",
    "    \n",
    "    # load raw csv\n",
    "    variable_db = pd.read_csv(path_to_raw_csv)\n",
    "    variable_db[\"GeoFIPS\"] = variable_db[\"GeoFIPS\"].astype(int)\n",
    "\n",
    "    # drop nans\n",
    "    variable_db = variable_db.dropna()\n",
    "\n",
    "    \n",
    "    if region_type == \"county\":\n",
    "        \n",
    "        # load gdb, to get list of current non-excluded FIPS codes\n",
    "        data = DataGrabber()\n",
    "        data.get_features_wide([\"gdp\"])\n",
    "        gdp = data.wide[\"gdp\"]\n",
    "        \n",
    "        # Check if there are any counties that are missing from variable_db but in exclusions_df\n",
    "        # If so, add them to exclusions, and re-run variable_db with new exclusions\n",
    "\n",
    "        if len(np.setdiff1d(gdp[\"GeoFIPS\"].unique(), variable_db[\"GeoFIPS\"].unique())) > 0:\n",
    "            # add new exclusions\n",
    "\n",
    "            new_exclusions = np.setdiff1d(\n",
    "                gdp[\"GeoFIPS\"].unique(), variable_db[\"GeoFIPS\"].unique()\n",
    "            )\n",
    "\n",
    "            print(\"Adding new exclusions to exclusions.csv: \" + str(new_exclusions))\n",
    "\n",
    "            # open exclusions file\n",
    "\n",
    "            exclusions = pd.read_csv(os.path.join(root, \"/data/raw/exclusions.csv\"))\n",
    "\n",
    "            new_rows = pd.DataFrame(\n",
    "                {\n",
    "                    \"dataset\": [variable_name] * len(new_exclusions),\n",
    "                    \"exclusions\": new_exclusions,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # Concatenate the new rows to the existing DataFrame\n",
    "            exclusions = pd.concat([exclusions, new_rows], ignore_index=True)\n",
    "\n",
    "            # Remove duplicates\n",
    "            exclusions = exclusions.drop_duplicates()\n",
    "\n",
    "            exclusions = exclusions.sort_values(by=[\"dataset\", \"exclusions\"]).reset_index(\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            exclusions.to_csv(\n",
    "                os.path.join(root, \"/data/raw/exclusions.csv\"), index=False\n",
    "            )\n",
    "\n",
    "            print(\"Rerunning gdp cleaning with new exclusions\")\n",
    "\n",
    "            # rerun gdp cleaning\n",
    "            clean_gdp()\n",
    "            clean_variable(variable_name, path_to_raw_csv)\n",
    "            return\n",
    "\n",
    "        # restrict to only common FIPS codes\n",
    "        common_fips = np.intersect1d(\n",
    "            gdp[\"GeoFIPS\"].unique(), variable_db[\"GeoFIPS\"].unique()\n",
    "        )\n",
    "        variable_db = variable_db[variable_db[\"GeoFIPS\"].isin(common_fips)]\n",
    "        variable_db = variable_db.merge(\n",
    "            gdp[[\"GeoFIPS\", \"GeoName\"]], on=[\"GeoFIPS\", \"GeoName\"], how=\"left\"\n",
    "        )\n",
    "        variable_db = variable_db.sort_values(by=[\"GeoFIPS\", \"GeoName\"])\n",
    "\n",
    "        # make sure that it passes this test data.wide[feature][column].dtype == float\n",
    "        for column in variable_db.columns:\n",
    "            if column not in [\"GeoFIPS\", \"GeoName\"]:\n",
    "                variable_db[column] = variable_db[column].astype(float)\n",
    "\n",
    "        # save 4 formats to .csv\n",
    "        variable_db_wide = variable_db.copy()\n",
    "        variable_db_long = pd.melt(\n",
    "            variable_db,\n",
    "            id_vars=[\"GeoFIPS\", \"GeoName\"],\n",
    "            var_name=YearOrCategory,\n",
    "            value_name=\"Value\",\n",
    "        )\n",
    "        variable_db_std_wide = standardize_and_scale(variable_db)\n",
    "        variable_db_std_long = pd.melt(\n",
    "            variable_db_std_wide.copy(),\n",
    "            id_vars=[\"GeoFIPS\", \"GeoName\"],\n",
    "            var_name=YearOrCategory,\n",
    "            value_name=\"Value\",\n",
    "        )\n",
    "        variable_db_wide.to_csv(\n",
    "            os.path.join(root, \"/data/processed/\" + variable_name + \"_wide.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        variable_db_long.to_csv(\n",
    "            os.path.join(root, \"/data/processed/\" + variable_name + \"_long.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        variable_db_std_wide.to_csv(\n",
    "            os.path.join(root, \"/data/processed/\" + variable_name + \"_std_wide.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "        variable_db_std_long.to_csv(\n",
    "            os.path.join(root, \"/data/processed/\" + variable_name + \"_std_long.csv\"),\n",
    "            index=False,\n",
    "        )\n",
    "\n",
    "\n",
    "    elif region_type == 'MA':\n",
    "        \n",
    "        metrolist = pd.read_csv(os.path.join(root, \"/data/raw/metrolist.csv\"))\n",
    "        \n",
    "        # dtypes\n",
    "        # \n",
    "        \n",
    "        # do something else\n",
    "        \n",
    "    else :\n",
    "        raise ValueError(\"region_type must be either 'county' or 'MA'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from cities.utils.cleaning_utils import standardize_and_scale\n",
    "from cities.utils.data_grabber import DataGrabber\n",
    "from cities.utils.clean_gdp import clean_gdp\n",
    "\n",
    "class VariableCleaner:\n",
    "    def __init__(self, variable_name, path_to_raw_csv, YearOrCategory=\"Year\", region_type=\"county\"):\n",
    "        self.variable_name = variable_name\n",
    "        self.path_to_raw_csv = path_to_raw_csv\n",
    "        self.YearOrCategory = YearOrCategory\n",
    "        self.region_type = region_type\n",
    "        self.root = find_repo_root()\n",
    "        self.data_grabber = DataGrabber()\n",
    "        self.gdp = None\n",
    "        self.variable_db = None\n",
    "\n",
    "    def clean_variable(self):\n",
    "        self.load_raw_csv()\n",
    "        self.drop_nans()\n",
    "        if self.region_type == \"county\":\n",
    "            self.load_gdp_data()\n",
    "            self.check_exclusions()\n",
    "            self.restrict_common_fips()\n",
    "            self.save_csv_files(self.region_type)\n",
    "        elif self.region_type == \"MA\":\n",
    "            self.process_MA_data()\n",
    "            \n",
    "            # self.check_exclusions('MA') functionality to implement in the future\n",
    "            self.save_csv_files(self.region_type)\n",
    "        else:\n",
    "            raise ValueError(\"region_type must be either 'county' or 'MA'\")\n",
    "\n",
    "    def load_raw_csv(self):\n",
    "        self.variable_db = pd.read_csv(self.path_to_raw_csv)\n",
    "        self.variable_db[\"GeoFIPS\"] = self.variable_db[\"GeoFIPS\"].astype(int)\n",
    "\n",
    "    def drop_nans(self):\n",
    "        self.variable_db = self.variable_db.dropna()\n",
    "        \n",
    "    def load_metro_areas(self):\n",
    "        self.metro = pd.read_csv(f\"{self.root}/data/raw/metrolist.csv\")\n",
    "\n",
    "    def load_gdp_data(self):\n",
    "        self.data_grabber.get_features_wide([\"gdp\"])\n",
    "        self.gdp = self.data_grabber.wide[\"gdp\"]\n",
    "\n",
    "    def check_exclusions(self):\n",
    "        common_fips = np.intersect1d(self.gdp[\"GeoFIPS\"].unique(), self.variable_db[\"GeoFIPS\"].unique())\n",
    "        if len(np.setdiff1d(self.gdp[\"GeoFIPS\"].unique(), self.variable_db[\"GeoFIPS\"].unique())) > 0:\n",
    "            self.add_new_exclusions(common_fips)\n",
    "            clean_gdp()\n",
    "            self.clean_variable()\n",
    "\n",
    "    def add_new_exclusions(self, common_fips):\n",
    "        new_exclusions = np.setdiff1d(self.gdp[\"GeoFIPS\"].unique(), self.variable_db[\"GeoFIPS\"].unique())\n",
    "        print(\"Adding new exclusions to exclusions.csv: \" + str(new_exclusions))\n",
    "        exclusions = pd.read_csv(os.path.join(self.root, \"/data/raw/exclusions.csv\"))\n",
    "        new_rows = pd.DataFrame({\"dataset\": [self.variable_name] * len(new_exclusions), \"exclusions\": new_exclusions})\n",
    "        exclusions = pd.concat([exclusions, new_rows], ignore_index=True)\n",
    "        exclusions = exclusions.drop_duplicates()\n",
    "        exclusions = exclusions.sort_values(by=[\"dataset\", \"exclusions\"]).reset_index(drop=True)\n",
    "        exclusions.to_csv(os.path.join(self.root, \"/data/raw/exclusions.csv\"), index=False)\n",
    "        print(\"Rerunning gdp cleaning with new exclusions\")\n",
    "\n",
    "\n",
    "    def restrict_common_fips(self):\n",
    "        common_fips = np.intersect1d(self.gdp[\"GeoFIPS\"].unique(), self.variable_db[\"GeoFIPS\"].unique())\n",
    "        self.variable_db = self.variable_db[self.variable_db[\"GeoFIPS\"].isin(common_fips)]\n",
    "        self.variable_db = self.variable_db.merge(self.gdp[[\"GeoFIPS\", \"GeoName\"]], on=[\"GeoFIPS\", \"GeoName\"], how=\"left\")\n",
    "        self.variable_db = self.variable_db.sort_values(by=[\"GeoFIPS\", \"GeoName\"])\n",
    "        for column in self.variable_db.columns:\n",
    "            if column not in [\"GeoFIPS\", \"GeoName\"]:\n",
    "                self.variable_db[column] = self.variable_db[column].astype(float)\n",
    "                \n",
    "    def process_MA_data(self):\n",
    "        \n",
    "        metro_areas = self.load_metro_areas()\n",
    "        assert metro_areas['GeoFIPS'].nunique() == self.variable_db['GeoFIPS'].nunique()\n",
    "        self.variable_db['GeoFIPS'] = self.variable_db['GeoFIPS'].astype(np.int64)\n",
    "        \n",
    "    def save_csv_files(self, regions):\n",
    "        \n",
    "        # make sure that a db is wide, if not make it wide\n",
    "        \n",
    "        if regions == 'county':\n",
    "            variable_db_wide = self.variable_db.copy()\n",
    "            variable_db_long = pd.melt(self.variable_db, id_vars=[\"GeoFIPS\", \"GeoName\"], var_name=self.YearOrCategory, value_name=\"Value\")\n",
    "            variable_db_std_wide = standardize_and_scale(self.variable_db)\n",
    "            variable_db_std_long = pd.melt(variable_db_std_wide.copy(), id_vars=[\"GeoFIPS\", \"GeoName\"], var_name=self.YearOrCategory, value_name=\"Value\")\n",
    "            variable_db_wide.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"_wide.csv\"), index=False)\n",
    "            variable_db_long.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"_long.csv\"), index=False)\n",
    "            variable_db_std_wide.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"_std_wide.csv\"), index=False)\n",
    "            variable_db_std_long.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"_std_long.csv\"), index=False)\n",
    "            \n",
    "        elif regions == 'MA':\n",
    "            \n",
    "            variable_db_wide = self.variable_db.copy()\n",
    "            variable_db_long = pd.melt(self.variable_db, id_vars=[\"GeoFIPS\", \"GeoName\"], var_name=self.YearOrCategory, value_name=\"Value\")\n",
    "            variable_db_std_wide = standardize_and_scale(self.variable_db)\n",
    "            variable_db_std_long = pd.melt(variable_db_std_wide.copy(), id_vars=[\"GeoFIPS\", \"GeoName\"], var_name=self.YearOrCategory, value_name=\"Value\")\n",
    "            variable_db_wide.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"ma_wide.csv\"), index=False)\n",
    "            variable_db_long.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"ma_long.csv\"), index=False)\n",
    "            variable_db_std_wide.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"ma_std_wide.csv\"), index=False)\n",
    "            variable_db_std_long.to_csv(os.path.join(self.root, \"/data/processed/\" + self.variable_name + \"ma_std_long.csv\"), index=False)\n",
    "            \n",
    "        else :\n",
    "            raise ValueError(\"region_type must be either 'county' or 'MA'\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use example\n",
    "\n",
    "cleaner = VariableCleaner(variable_name=\"example_variable\",\n",
    "                          path_to_raw_csv=\"path/to/raw/csv/file.csv\", YearOrCategory=\"Year\", region_type=\"county\")\n",
    "cleaner.clean_variable()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
